{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgboost\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: TimeSeries approach groupby sales date -> merge with google data\r\n",
    "(Assumption: Avocado King wants to see sale price per day )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, groupby date and keep date, price, volume sales only\r\n",
    "price_df = pd.read_csv('Data/price-and-sales-data.csv')\r\n",
    "\r\n",
    "price_df = price_df[['Date', 'TotalVolume', '4046', '4225', '4770', 'AveragePrice']]\r\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values with median\r\n",
    "(one practice since only numeric columns selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.fillna(price_df.median(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df['Date'] = pd.to_datetime(price_df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales = price_df.groupby('Date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales['AveragePrice'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data = pd.read_csv('Data/google-data.csv')\r\n",
    "google_data['Week'] = pd.to_datetime(google_data['Week'])\r\n",
    "google_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(weekly_sales, google_data, how='outer', left_on='Date', right_on='Week')\r\n",
    "combined_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason missing values with merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[combined_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection based on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "sns.heatmap(np.abs(combined_df.corr()),annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.abs(combined_df.corr())['AveragePrice'] > 0.3\r\n",
    "column_df = pd.DataFrame(a)\r\n",
    "keep_cols = list(column_df[column_df.AveragePrice==True].index.values)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_timeseries(input_df, test_start=2019, index_col='Week', output_col='AveragePrice', keep_cols=None):    \n",
    "    set_df = input_df.set_index(index_col)\n",
    "    if keep_cols:\n",
    "        set_df = set_df[keep_cols]\n",
    "\n",
    "    X = set_df.drop([output_col], axis=1)\n",
    "    y = set_df[output_col]\n",
    "\n",
    "    X_train = X[:str(test_start-1)]\n",
    "    y_train = y[:str(test_start-1)]\n",
    "    y_test = y[str(test_start):]\n",
    "    X_test = X[str(test_start):]    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_timeseries(combined_df, test_start=2017, keep_cols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "scaler = MinMaxScaler()\r\n",
    "scaled_Xtrain = scaler.fit_transform(X_train)\r\n",
    "scaled_Xtest = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(scaled_Xtrain, y_train)\n",
    "y_predicted = lr.predict(scaled_Xtest)\n",
    "print('r2 score', r2_score(y_predicted, y_test.values))\n",
    "result_plot(y_test.values, y_predicted, 'Timeseries regression')\n",
    "sns.displot(np.abs((y_predicted-y_test.values)), label='Residual plot')\n",
    "y_predicted_df = pd.DataFrame(y_predicted)\n",
    "y_predicted_df.index = y_test.index\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(y_test)\n",
    "plt.plot(y_predicted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "alphas = [1e-3, 1e-2, .1, 1, 10, 1e2]\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_test)\n",
    "labels = ['True value']\n",
    "for (ii, alpha) in enumerate(alphas):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(scaled_Xtrain, y_train)\n",
    "    y_predicted = model.predict(scaled_Xtest)\n",
    "    y_predicted_df = pd.DataFrame(y_predicted)\n",
    "    y_predicted_df.index = y_test.index\n",
    "    print(alpha, r2_score(y_test.values, y_predicted), mean_squared_error(y_test.values, y_predicted))\n",
    "    plt.plot(y_predicted_df)\n",
    "    labels.append('Model with alpha='+str(alpha))\n",
    "\n",
    "plt.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\r\n",
    "Good for recognising the trend but not really predict the exact price. Be careful to input great incidence into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Tabular approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = pd.read_csv('Data/price-and-sales-data.csv')\r\n",
    "price_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values with mean of that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep column names with missing values\r\n",
    "checking_missing_values = price_df.isnull().any()\r\n",
    "missing_value_cols = checking_missing_values[checking_missing_values == True].index.values\r\n",
    "missing_value_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_value_cols:\r\n",
    "    if price_df[col].dtypes != 'object': # double check dtype is NOT object (categorical)\r\n",
    "        price_df[col] = price_df.groupby('Date')[col].transform(lambda x : x.fillna(x.mean()))\r\n",
    "price_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_df.describe())\r\n",
    "print(price_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_data = pd.read_csv('Data/google-data.csv')\r\n",
    "google_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(price_df, google_data, how='inner', left_on='Date', right_on='Week')\r\n",
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis of combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\r\n",
    "sns.heatmap(combined_df.corr(), cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['type', 'region']\r\n",
    "combined_df[cols] = combined_df[cols].astype('category')\r\n",
    "onehot_df = pd.get_dummies(combined_df, drop_first=True, columns=cols)\r\n",
    "onehot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = onehot_df.drop(['Date', 'AveragePrice', 'Week'], axis=1)\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit_transform(X)\n",
    "\n",
    "var = pca.explained_variance_ratio_\n",
    "cumsum_var = np.cumsum(var)\n",
    "plt.plot(cumsum_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_avocado(input_df, test_size=0.3, random_state=42, pca_component_keep=None, min_max_scaling=False):\n",
    "    X = input_df.drop(['Date', 'AveragePrice', 'Week'], axis=1)\n",
    "    y = input_df['AveragePrice']\n",
    "    \n",
    "    if pca_component_keep:\n",
    "        pca = PCA(n_components = pca_component_keep)\n",
    "        X = pca.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if min_max_scaling:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "    print('Training shape: ', X_train.shape)\n",
    "    print('Testing shape: ', X_test.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_avocado(onehot_df, test_size=0.3, random_state=42, pca_component_keep=None, min_max_scaling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.metrics import r2_score, \\\n",
    "        explained_variance_score, mean_absolute_error, median_absolute_error, mean_squared_log_error, mean_squared_error\n",
    "\n",
    "def regression_results(y_true_a, y_pred_a):\n",
    "    all_positive = ((y_true_a >= 0).all() and (y_pred_a >= 0).all())\n",
    "    # Regression metrics\n",
    "    l_explained_variance=explained_variance_score(y_true_a, y_pred_a)\n",
    "    l_mean_absolute_error=mean_absolute_error(y_true_a, y_pred_a)\n",
    "    l_mean_squared=mean_squared_error(y_true_a, y_pred_a)\n",
    "    l_median_absolute_error=median_absolute_error(y_true_a, y_pred_a)\n",
    "    l_r2=r2_score(y_true_a, y_pred_a)\n",
    "\n",
    "    print('explained_variance: ', round(l_explained_variance,4))\n",
    "    print('r2: ', round(l_r2,4))\n",
    "    print('MAE: ', round(l_mean_absolute_error,4))\n",
    "    print('MSE: ', round(l_mean_squared,4))\n",
    "    print('RMSE: ', round(np.sqrt(l_mean_squared),4))\n",
    "    print('median_absolute_error: ', round(l_median_absolute_error,4))\n",
    "    if (all_positive):\n",
    "        l_mean_squared_log_error=mean_squared_log_error(y_true_a, y_pred_a)\n",
    "        print('mean_squared_log_error: ', round(l_mean_squared_log_error,4))\n",
    "def result_plot(y_test_b, y_pred_b, name):\n",
    "    plt.figure(figsize=(40,10))\n",
    "    plt.plot(y_pred_b, 'ro', label='prediction')\n",
    "    plt.plot(y_test_b,' go', label='ground truth')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    print(f'\\n {name}:')\n",
    "    regression_results(y_test_b, y_pred_b)\n",
    "    \n",
    "    matplotlib.rc('xtick', labelsize=15)\n",
    "    matplotlib.rc('ytick', labelsize=15)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(y_pred_b, y_test_b, 'ro')\n",
    "    plt.xlabel('Predictions', fontsize = 15)\n",
    "    plt.ylabel('Reality', fontsize = 15)\n",
    "    plt.title('Predictions x Reality on dataset', fontsize = 15)\n",
    "    ax.plot([y_pred_b.min(), y_pred_b.max()], [y_test_b.min(), y_test_b.max()], 'k--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dctr = DecisionTreeRegressor()\n",
    "dctr.fit(X_train, y_train)\n",
    "y_predicted = dctr.predict(X_test)\n",
    "result_plot(y_test.values, y_predicted, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = xgboost.XGBRegressor()\n",
    "xgbr.fit(X_train, y_train)\n",
    "y_predicted = xgbr.predict(X_test)\n",
    "result_plot(y_test.values, y_predicted, 'XGBoost Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance but only PCA is not applied\n",
    "# dic = {}\n",
    "# for (col, score) in zip(X_train.columns, xgbr.feature_importances_):\n",
    "#     dic[col] = [score]\n",
    "\n",
    "# ft_imp = pd.DataFrame.from_dict(dic)\n",
    "# ft_imp.iloc[0].plot(kind='bar', figsize=(16,8), title='Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_predicted = lr.predict(X_test)\n",
    "result_plot(y_test.values, y_predicted, 'Linear Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense ANN modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon())) \n",
    "\n",
    "def build_model(train_shape, optimizer='Adam', num_layers=1, num_perceptrons=10, activation='sigmoid'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=train_shape))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(num_perceptrons, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse', 'mae', coeff_determination])\n",
    "    return model\n",
    "\n",
    "model = build_model(X_train.shape[1], num_layers=4, num_perceptrons=50)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50)\n",
    "\n",
    "y_predicted = model.predict(X_train)\n",
    "result_plot(y_test.values, y_predicted, 'Dense ANN')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51cc139143aa370d9407f9a9844352f31e11ed4b1311e4ca6ee33076325ff02a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ds': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}